---
title: TSA Assignment 10
author: Christian Hilscher - 1570550
date: 25/5/2020
---

# Question 10

This assignment is coded in Julia since I figured more speed is always good. Unfortunately the format is not too nice to look since it's the first time using this language together with Markdown so sorry for that in advance.
## DGP

```julia; echo = false; results = "hidden"
using Statistics
using ProgressMeter
using Plots
using StatsPlots
```
The constants are given by

```julia;results="hidden"
const M=2000
const T=1000
const S=1000
const d=0.25
const d_list = LinRange(-0.45, 0.45, 19)
const α_list = LinRange(0.2, 0.8, 7)
```

First I generate the ``x_t`` with the following function where I assume ``e_0 = 0 ``.

```julia; results="hidden"
function get_x()
    # Initializing x_t
    x_t = Array{Float64}(undef, M+T, S)

    # Drawing errors
    e = randn(Float64, M+T+1, S)
    e[1,:] = zeros(S)   # Setting first error to zero by assumption

    # Generating x_t
    x_t[1,:] = e[1,:]
    for i = 2:(M+T)
        x_t[i,:] = 0.5*x_t[i-1,:] + e[i,:] + 0.3*e[i-1,:]
    end
    return x_t
end
```

Afterwards I get the ``\psi_j`` recursively and set ``\psi_0=1``. Noting that ``d=0.25`` and ``y_t = (1-L)^{-d} x_t`` I switch the sign for ``d`` from slide 156.

```julia; results="hidden"
function get_ψ(d)
    ψ = Array{Float64}(undef, M+T)
    ψ[1] = 1

    for j=1:M+T-1
        ψ[j+1] = ((j-1+d)/j) * ψ[j]
    end
    return ψ
end
```

The final time series ``y_t`` is then given by


```julia; results="hidden"
function get_data(x, ψ)

    y_final = Array{Float64}(undef, M+T, S)

    # Generating matrix to sum over
    xmat = Array{Float64}(undef, M+T, M+T)
    ymat = similar(xmat)

    y = Array{Float64}(undef, M+T)
    x_used = Array{Float64}(undef, M+T)
    x_tmp = zeros(M+T)

    @showprogress 1 "Generating Data..." for s=1:S
        # Running sample s
        x_used = x[:,s]

        for j=0:M+T-1
            x_tmp = zeros(M+T)
            x_tmp[j+1:end] = x_used[1:end-j]
            xmat[j+1,:] = x_tmp
        end

        ymat = xmat.*ψ

        for col=1:size(ymat)[2]
            y[col] = sum(ymat[:,col])
        end
        y_final[:,s] = y
    end
    return y_final
end
```

From this I discard the first ``M`` obsverations such that a time series with 1000 observations is left.

## Local Whittle Estimator

Here I follow the slides for building the objective function to be minimized. The function generating ``I_y(\lambda_j)`` takes as inputs a given ``\lambda_j`` and a time series ``x``. These are not the ``x_t`` from before which were used to calculate ``y_t`` but rather I refer to one of the 1000 samples of ``y_t`` by calling it ``x_t``. In other words, ``x_t`` is a vector containing ``T`` observations.

```julia;results="hidden"
function Iy(λj, x)
    bigT = length(x)
    prefactor = (1/2*π*bigT)

    # Making space
    firstpart = Array{Float64}(undef, bigT-1)
    secondpart = similar(firstpart)

    # Crunching numbers
    for t=1:bigT-1
        firstpart[t] = x[t] * cos(λj*(t-1))
        secondpart[t] = x[t] * sin(λj*(t-1))
    end
    value = prefactor * ((sum(firstpart))^2 + (sum(secondpart))^2)
    return value
end
```

The vector with all the ``\lambda_j`` is calculated by

```julia;results="hidden"
function get_λs(α, tslength)

    # Making space
    little_m = floor(Int,tslength^α)
    λs = Array{Float64}(undef, little_m)

    # Getting lambdas
    for m=1:little_m
        λs[m] = (2*π/tslength) * m
    end
    return λs, little_m
end
```

As soon as I have both ``\lambda_j`` as well as ``I_y(\lambda_j)`` the objective function is given by

$$
R(d) = log \left( \frac{1}{m} \sum_{j=1}^m \frac{I_y(\lambda_j)}{\lambda_j^{-2d}} \right) - \frac{2d}{m} \left( \sum_{j=1}^m log(\lambda_j) \right)
$$

The corresponding code is

```julia;results="hidden"
function objf(α, x, d)
    λ_list, little_m = get_λs(α, length(x))

    first_sum = Array{Float64}(undef, little_m)
    second_sum = similar(first_sum)

    for (m, λ) in enumerate(λ_list)
        first_sum[m] = Iy(λ, x) / λ^(-2*d)
        second_sum[m] = log(λ)
    end

    firstpart = (1/little_m) * sum(first_sum)
    secondpart = (2*d/little_m) * sum(second_sum)

    value = log(firstpart) - secondpart
    return value
end
```



The optimization over ``d`` is then implemented by



```julia;results="hidden"
function optimize_overd(α, x, ds)
    values = Array{Float64}(undef, length(ds))

    for (i, d) in enumerate(ds)
        values[i] = objf(α, x, d)
    end
    min_d = d_list[argmin(values)]
    return min_d
end
```

## Periodogram-Regression

As an alternative approach I also use the log PR which is given by

$$
I_j = \alpha_j + d R_j + v_j \\
$$

with

* ``I_j = log(I_y(\lambda_j))``
* ``R_j = -log(r_j)`` where ``r_j = 4 \sin^2(\frac{\lambda_j}{2})``

The functions used for this approach are the following:

```julia;results="hidden"
function Ij(λj, x)
    value = Iy(λj, x)
    return log(value)
end

function Rj(λj)
    r = 4 * sin((λj/2))^2
    return -log(r)
end

function OLSestimator(x, y)
    res = inv(x'*x)*(x'*y)
    return res
end
```

The estimation results for a particular ``d`` are then given by

```julia;results="hidden"
function PR(α, x)
    λ_list, little_m = get_λs(α, length(x))

    x_values = Array{Float64}(undef, little_m, 2)
    y_values = Array{Float64}(undef, little_m, 1)

    x_values[:,1] = ones(little_m)

    for (m, λ) in enumerate(λ_list)
        x_values[m,2] = Rj(λ)
        y_values[m] = Ij(λ, x)
    end

    estimtates = OLSestimator(x_values, y_values)
    return estimtates[2]
end
```

## Optimization

I supply to grids: one containing the values for ``\alpha`` and one for ``d``. For each of the ``\alpha`` I loop over all values in the ``d\_list``. For the periodogram I save those values which minimize the objective function and for the PR the estimation results of ``d`` are saved.

```julia;results="hidden"
function optimize_overα(αs, x, ds)
    # Making space
    values_LW = Array{Float64}(undef, size(x)[2], length(αs))
    values_PR = similar(values_LW)

    # Getting values
    for (i, α) in enumerate(αs)
        for sample=1:size(x)[2]
            values_LW[sample, i] = optimize_overd(α, x[:,sample], ds)
            values_PR[sample, i] = PR(α, x[:,sample])
        end
        println(α)
    end
    return values_LW, values_PR
end

function optimize_overd(α, x, ds)
    # For LW
    values = Array{Float64}(undef, length(ds))

    for (i, d) in enumerate(ds)
        values[i] = objf(α, x, d)
    end
    min_d = d_list[argmin(values)]
    return min_d
end
```

## Analysis

To get the coverage probabilites as well as some insights into the behaviour of the bias and variance depending on a chosen parameter ``\alpha`` I run the following chunk:

```julia;results="hidden"
function cov_probs(LW_values, PR_values, αs)
    # Making space
    probs = Array{Float64}(undef, 2, size(LW_values)[2])
    biases = similar(probs)
    vars = similar(probs)

    # Getting values
    for (index, df) in enumerate([LW_values, PR_values])
        for i=1:size(LW_values)[2]
            α = αs[i]
            probs[index,i] = calc_share(df[:,i], α, index)
            biases[index,i] = calc_bias(df[:,i])
            vars[index,i] = calc_sigma2(df[:,i])
        end
    end
    return probs, biases, vars
end
```

The coverage probabilites depend on ``\alpha`` and whether the LW or PR is implemented since their asymptotic varainces differ. First I get the CIs for each approach and then calculate how many of the 1000 samples actually yield an estimate for ``d`` inside those CIs.

```julia;results="hidden"
function get_cis(n, α, type)
    """
    Type 1 = LW, Type 2 = PR
    """
    # Asymptotc variance depends on α
    m = floor(T^α)
    z = 1.96    # Choosing 5% level
    if type == 1
        sigma = sqrt(1/(4*m))
    else
        sigma = sqrt(((π^2)/(24*m)))
    end

    ci_min = d - (z*sigma)/sqrt(n)
    ci_max = d + (z*sigma)/sqrt(n)

    return (ci_min, ci_max)
end

function calc_share(x, α, type)
    """
    Type 1 = LW, Type 2 = PR
    """

    n = length(x)
    ci_low, ci_high = get_cis(n, α, type)

    positives = sum([(element > ci_low) & (element < ci_high) for element in x])
    value = positives/n
    return value
end
```

The bias and variance of each sample are simply

```julia;results="hidden"
function calc_bias(x)
    bias = mean(x) - d
    return bias
end

function calc_sigma2(x)
    σ2 = var(x)
    return σ2
end
```

## Results

```julia;echo=false;results="hidden"
x_one = get_x()
ψ_one = get_ψ(d)

y = get_data(x_one, ψ_one)
y_use = y[M+1:end,:]


# Optimizing
df_LW, df_PR = optimize_overα(α_list, y_use, d_list)
# Calculating final results
co_prob, b, sigma = cov_probs(df_LW, df_PR, α_list)
```


The coverage probabilites are  are given by

|  α  |   LW  |   PR  |
|-----|-------|-------|
| 0.2 | 0.030 | 0.020 |
| 0.3 | 0.055 | 0.032 |
| 0.4 | 0.098 | 0.053 |
| 0.5 | 0.176 | 0.038 |
| 0.6 | 0.241 | 0.031 |
| 0.7 | 0.026 | 0.007 |
| 0.8 | 0.000 | 0.000 |





It is evident that the the LW estimator has the higher coverage probabilities.
The bias and variances behave the following way

```julia;echo=false
plot(α_list, abs.(b)', title="Biases depending on α", label = ["LW" "PR"], lw=2)
```

```julia;echo=false

plot(α_list, sigma', title="Variances depending on α", label = ["LW" "PR"], lw=2)
```

Finally when looking at them MSE one can observe that at least in terms of the MSE the LW always outperforms the PR

```julia;echo=false
MSE = b.^2 .+ sigma

plot(α_list, MSE', title="MSE depending on α", label = ["LW" "PR"], lw=2)
```
